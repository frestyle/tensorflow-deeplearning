{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#Read the csv file\n",
    "df = pd.read_csv('data/AAPL.csv')\n",
    "train_dates = pd.to_datetime(df['date'])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\n",
    "#Variables for training\n",
    "cols = ['close','volume']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#New dataframe with only training data - 5 columns\n",
    "df_for_training = df[cols].astype(float)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n",
    "# normalize the dataset\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(df_for_training)\n",
    "df_for_training_scaled = scaler.transform(df_for_training)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "n_train_days = 52   # Number of days we want to remove from test\n",
    "\n",
    "X_train = df_for_training_scaled[: df_for_training_scaled.shape[0] - n_train_days] \n",
    "X_days =  train_dates[: df_for_training_scaled.shape[0] - n_train_days] \n",
    "\n",
    "y_train = df_for_training_scaled[df_for_training_scaled.shape[0] - n_train_days:] \n",
    "y_days =  train_dates[df_for_training_scaled.shape[0] - n_train_days:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "\n",
    "#As required for LSTM networks, we require to reshape an input data into n_samples x timesteps x n_features. \n",
    "#In this example, the n_features is 5. We will make timesteps = 14 (past days data used for training). \n",
    "\n",
    "#Empty lists to be populated using formatted training data\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "n_future = 1   # Number of days we want to look into the future based on the past days.\n",
    "n_past = 14  # Number of past days we want to use to predict the future.\n",
    "\n",
    "#Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
    "#In my example, my df_for_training_scaled has a shape (12823, 5)\n",
    "#12823 refers to the number of data points and 5 refers to the columns (multi-variables).\n",
    "for i in range(n_past, len(X_train) - n_future +1):\n",
    "    trainX.append(X_train[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "    trainY.append(X_train[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "\n",
    "trainX, trainY = np.array(trainX), np.array(trainY)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In my case, trainX has a shape (12809, 14, 5). \n",
    "\n",
    "12809 because we are looking back 14 days (12823 - 14 = 12809). \n",
    "\n",
    "Remember that we cannot look back 14 days until we get to the 15th day. \n",
    "\n",
    "Also, trainY has a shape (12809, 1). Our model only predicts a single value, but \n",
    "\n",
    "it needs multiple variables (5 in my example) to make this prediction. \n",
    "\n",
    "This is why we can only predict a single day after our training, the day after where our data ends.\n",
    "\n",
    "To predict more days in future, we need all the 5 variables which we do not have. \n",
    "\n",
    "We need to predict all variables if we want to do that. \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# define the Autoencoder model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(64, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(trainY.shape[1]))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 14, 128)           67072     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 14, 64)            49408     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 128,929\n",
      "Trainable params: 128,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# fit the model\n",
    "history = model.fit(trainX, trainY, epochs=125, batch_size=16, validation_split=0.1, verbose=1)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/125\n",
      "148/148 [==============================] - 12s 62ms/step - loss: 0.0379 - val_loss: 3.7665\n",
      "Epoch 2/125\n",
      "148/148 [==============================] - 8s 55ms/step - loss: 0.0119 - val_loss: 0.3737\n",
      "Epoch 3/125\n",
      "148/148 [==============================] - 8s 56ms/step - loss: 0.0112 - val_loss: 0.1834\n",
      "Epoch 4/125\n",
      "148/148 [==============================] - 8s 54ms/step - loss: 0.0096 - val_loss: 0.5671\n",
      "Epoch 5/125\n",
      "148/148 [==============================] - 8s 54ms/step - loss: 0.0102 - val_loss: 0.3652\n",
      "Epoch 6/125\n",
      "148/148 [==============================] - 8s 53ms/step - loss: 0.0085 - val_loss: 0.1280\n",
      "Epoch 7/125\n",
      "148/148 [==============================] - 7s 46ms/step - loss: 0.0088 - val_loss: 0.0422\n",
      "Epoch 8/125\n",
      "148/148 [==============================] - 7s 46ms/step - loss: 0.0094 - val_loss: 0.6826\n",
      "Epoch 9/125\n",
      "148/148 [==============================] - 7s 47ms/step - loss: 0.0090 - val_loss: 0.0722\n",
      "Epoch 10/125\n",
      "148/148 [==============================] - 7s 46ms/step - loss: 0.0081 - val_loss: 0.5676\n",
      "Epoch 11/125\n",
      "148/148 [==============================] - 7s 44ms/step - loss: 0.0080 - val_loss: 1.0122\n",
      "Epoch 12/125\n",
      "148/148 [==============================] - 7s 46ms/step - loss: 0.0089 - val_loss: 0.4329\n",
      "Epoch 13/125\n",
      "148/148 [==============================] - 7s 47ms/step - loss: 0.0079 - val_loss: 0.2022\n",
      "Epoch 14/125\n",
      "148/148 [==============================] - 7s 47ms/step - loss: 0.0075 - val_loss: 0.7623\n",
      "Epoch 15/125\n",
      "148/148 [==============================] - 8s 51ms/step - loss: 0.0083 - val_loss: 0.0963\n",
      "Epoch 16/125\n",
      "148/148 [==============================] - 7s 50ms/step - loss: 0.0073 - val_loss: 0.2541\n",
      "Epoch 17/125\n",
      "148/148 [==============================] - 8s 54ms/step - loss: 0.0080 - val_loss: 0.3165\n",
      "Epoch 18/125\n",
      "148/148 [==============================] - 8s 57ms/step - loss: 0.0076 - val_loss: 0.5120\n",
      "Epoch 19/125\n",
      "148/148 [==============================] - 8s 57ms/step - loss: 0.0077 - val_loss: 0.7249\n",
      "Epoch 20/125\n",
      "148/148 [==============================] - 9s 58ms/step - loss: 0.0080 - val_loss: 0.3674\n",
      "Epoch 21/125\n",
      "148/148 [==============================] - 8s 53ms/step - loss: 0.0068 - val_loss: 0.2999\n",
      "Epoch 22/125\n",
      "148/148 [==============================] - 8s 54ms/step - loss: 0.0072 - val_loss: 0.3686\n",
      "Epoch 23/125\n",
      "148/148 [==============================] - 7s 47ms/step - loss: 0.0075 - val_loss: 0.4126\n",
      "Epoch 24/125\n",
      "148/148 [==============================] - 7s 45ms/step - loss: 0.0078 - val_loss: 0.4785\n",
      "Epoch 25/125\n",
      "148/148 [==============================] - 8s 55ms/step - loss: 0.0076 - val_loss: 0.3717\n",
      "Epoch 26/125\n",
      "148/148 [==============================] - 7s 48ms/step - loss: 0.0068 - val_loss: 0.5866\n",
      "Epoch 27/125\n",
      "148/148 [==============================] - 8s 53ms/step - loss: 0.0067 - val_loss: 0.3800\n",
      "Epoch 28/125\n",
      "148/148 [==============================] - 8s 52ms/step - loss: 0.0062 - val_loss: 0.3359\n",
      "Epoch 29/125\n",
      "148/148 [==============================] - 7s 49ms/step - loss: 0.0068 - val_loss: 0.5444\n",
      "Epoch 30/125\n",
      "148/148 [==============================] - 7s 47ms/step - loss: 0.0065 - val_loss: 0.4778\n",
      "Epoch 31/125\n",
      "148/148 [==============================] - 7s 49ms/step - loss: 0.0069 - val_loss: 0.3617\n",
      "Epoch 32/125\n",
      "148/148 [==============================] - 7s 45ms/step - loss: 0.0064 - val_loss: 0.6499\n",
      "Epoch 33/125\n",
      "148/148 [==============================] - 8s 51ms/step - loss: 0.0072 - val_loss: 0.6387\n",
      "Epoch 34/125\n",
      "148/148 [==============================] - 8s 53ms/step - loss: 0.0067 - val_loss: 0.3997\n",
      "Epoch 35/125\n",
      "148/148 [==============================] - 8s 57ms/step - loss: 0.0069 - val_loss: 0.3667\n",
      "Epoch 36/125\n",
      "148/148 [==============================] - 9s 57ms/step - loss: 0.0060 - val_loss: 0.3133\n",
      "Epoch 37/125\n",
      "148/148 [==============================] - 9s 58ms/step - loss: 0.0065 - val_loss: 0.1732\n",
      "Epoch 38/125\n",
      "148/148 [==============================] - 9s 58ms/step - loss: 0.0064 - val_loss: 0.1567\n",
      "Epoch 39/125\n",
      "148/148 [==============================] - 8s 56ms/step - loss: 0.0069 - val_loss: 0.2691\n",
      "Epoch 40/125\n",
      "148/148 [==============================] - 8s 57ms/step - loss: 0.0069 - val_loss: 0.3940\n",
      "Epoch 41/125\n",
      "148/148 [==============================] - 8s 57ms/step - loss: 0.0063 - val_loss: 0.0642\n",
      "Epoch 42/125\n",
      "148/148 [==============================] - 8s 57ms/step - loss: 0.0064 - val_loss: 0.4528\n",
      "Epoch 43/125\n",
      "148/148 [==============================] - 9s 58ms/step - loss: 0.0068 - val_loss: 0.3696\n",
      "Epoch 44/125\n",
      "148/148 [==============================] - 8s 54ms/step - loss: 0.0061 - val_loss: 0.4745\n",
      "Epoch 45/125\n",
      "148/148 [==============================] - 8s 57ms/step - loss: 0.0059 - val_loss: 0.3239\n",
      "Epoch 46/125\n",
      "148/148 [==============================] - 9s 58ms/step - loss: 0.0066 - val_loss: 0.3170\n",
      "Epoch 47/125\n",
      "148/148 [==============================] - 9s 60ms/step - loss: 0.0066 - val_loss: 0.2247\n",
      "Epoch 48/125\n",
      "148/148 [==============================] - 8s 53ms/step - loss: 0.0067 - val_loss: 0.2475\n",
      "Epoch 49/125\n",
      "148/148 [==============================] - 8s 57ms/step - loss: 0.0062 - val_loss: 0.2054\n",
      "Epoch 50/125\n",
      "148/148 [==============================] - 9s 62ms/step - loss: 0.0057 - val_loss: 0.1536\n",
      "Epoch 51/125\n",
      "148/148 [==============================] - 9s 64ms/step - loss: 0.0069 - val_loss: 0.1065\n",
      "Epoch 52/125\n",
      "148/148 [==============================] - 10s 68ms/step - loss: 0.0063 - val_loss: 0.4100\n",
      "Epoch 53/125\n",
      "148/148 [==============================] - 9s 64ms/step - loss: 0.0059 - val_loss: 0.2443\n",
      "Epoch 54/125\n",
      "148/148 [==============================] - 10s 65ms/step - loss: 0.0068 - val_loss: 0.3035\n",
      "Epoch 55/125\n",
      "148/148 [==============================] - 10s 65ms/step - loss: 0.0065 - val_loss: 0.2108\n",
      "Epoch 56/125\n",
      "148/148 [==============================] - 10s 65ms/step - loss: 0.0061 - val_loss: 0.0878\n",
      "Epoch 57/125\n",
      "148/148 [==============================] - 10s 65ms/step - loss: 0.0065 - val_loss: 0.0911\n",
      "Epoch 58/125\n",
      "148/148 [==============================] - 9s 62ms/step - loss: 0.0063 - val_loss: 0.1365\n",
      "Epoch 59/125\n",
      "148/148 [==============================] - 9s 63ms/step - loss: 0.0059 - val_loss: 0.1213\n",
      "Epoch 60/125\n",
      "148/148 [==============================] - 10s 65ms/step - loss: 0.0061 - val_loss: 0.0664\n",
      "Epoch 61/125\n",
      "148/148 [==============================] - 10s 65ms/step - loss: 0.0059 - val_loss: 0.1877\n",
      "Epoch 62/125\n",
      "148/148 [==============================] - 10s 64ms/step - loss: 0.0068 - val_loss: 0.4356\n",
      "Epoch 63/125\n",
      "148/148 [==============================] - 10s 64ms/step - loss: 0.0062 - val_loss: 0.4788\n",
      "Epoch 64/125\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0065 - val_loss: 0.2200\n",
      "Epoch 65/125\n",
      "148/148 [==============================] - 9s 64ms/step - loss: 0.0062 - val_loss: 0.1379\n",
      "Epoch 66/125\n",
      "148/148 [==============================] - 9s 64ms/step - loss: 0.0065 - val_loss: 0.2620\n",
      "Epoch 67/125\n",
      "148/148 [==============================] - 9s 63ms/step - loss: 0.0060 - val_loss: 0.1662\n",
      "Epoch 68/125\n",
      "148/148 [==============================] - 10s 65ms/step - loss: 0.0066 - val_loss: 0.2721\n",
      "Epoch 69/125\n",
      "148/148 [==============================] - 10s 65ms/step - loss: 0.0062 - val_loss: 0.1867\n",
      "Epoch 70/125\n",
      "148/148 [==============================] - 9s 64ms/step - loss: 0.0062 - val_loss: 0.1957\n",
      "Epoch 71/125\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0052 - val_loss: 0.2121\n",
      "Epoch 72/125\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0059 - val_loss: 0.4053\n",
      "Epoch 73/125\n",
      "148/148 [==============================] - 10s 68ms/step - loss: 0.0062 - val_loss: 0.2372\n",
      "Epoch 74/125\n",
      "148/148 [==============================] - 10s 64ms/step - loss: 0.0060 - val_loss: 0.3131\n",
      "Epoch 75/125\n",
      "148/148 [==============================] - 9s 64ms/step - loss: 0.0066 - val_loss: 0.2544\n",
      "Epoch 76/125\n",
      "148/148 [==============================] - 8s 57ms/step - loss: 0.0060 - val_loss: 0.0241\n",
      "Epoch 77/125\n",
      "148/148 [==============================] - 8s 55ms/step - loss: 0.0059 - val_loss: 0.1533\n",
      "Epoch 78/125\n",
      " 85/148 [================>.............] - ETA: 3s - loss: 0.0061"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c36df8aab0f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "#Predicting...\n",
    "#Libraries that will help us extract only business days in the US.\n",
    "#Otherwise our dates would be wrong when we look back (or forward).  \n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
    "\n",
    "# recompute entire data set \n",
    "\n",
    "#Empty lists to be populated using formatted training data\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "#Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
    "#In my example, my df_for_training_scaled has a shape (12823, 5)\n",
    "#12823 refers to the number of data points and 5 refers to the columns (multi-variables).\n",
    "for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n",
    "    trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "    trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "\n",
    "trainX, trainY = np.array(trainX), np.array(trainY)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Remember that we can only predict one day in future as our model needs 5 variables\n",
    "#as inputs for prediction. We only have all 5 variables until the last day in our dataset.\n",
    "n_past = 201\n",
    "n_days_for_prediction=200  #let us predict past 15 days\n",
    "\n",
    "predict_period_dates = pd.date_range(list(train_dates)[-n_past], periods=n_days_for_prediction, freq=us_bd).tolist()\n",
    "\n",
    "#Make prediction\n",
    "prediction = model.predict(trainX[-n_days_for_prediction:]) #shape = (n, 1) where n is the n_days_for_prediction\n",
    "\n",
    "#Perform inverse transformation to rescale back to original range\n",
    "#Since we used 5 variables for transform, the inverse expects same dimensions\n",
    "#Therefore, let us copy our values 5 times and discard them after inverse transform\n",
    "prediction_copies = np.repeat(prediction, df_for_training.shape[1], axis=-1)\n",
    "y_pred_future = scaler.inverse_transform(prediction_copies)[:,0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Convert timestamp to date\n",
    "forecast_dates = []\n",
    "for time_i in predict_period_dates:\n",
    "    forecast_dates.append(time_i.date())\n",
    "\n",
    "\n",
    "df_forecast = pd.DataFrame({'date':np.array(forecast_dates), 'closeadj':y_pred_future})\n",
    "df_forecast['date']=pd.to_datetime"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "plt.title(label=\"Apple Anomalies\")\n",
    "# # plt.plot(rx_anomaly, ry_anomaly, \"gX\", markersize=10)\n",
    "# # plt.plot(x_anomaly, y_anomaly, \"r*\", markersize=5)\n",
    "# # plt.bar(dates,aapl.Volume,label=\"Volume\")\n",
    "plt.plot(forecast_dates, df_forecast.closeadj, label=\"predicted value\")\n",
    "plt.plot(train_dates, df.closeadj, label=\"Prices\")\n",
    "# plt.xlabel(\"Time\", fontsize=20)\n",
    "# plt.ylabel(\"volume\", fontsize=20)\n",
    "plt.legend()\n",
    "# # Tweak spacing to prevent clipping of ylabel\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# (df_forecast['date'])\n",
    "\n",
    "\n",
    "# original = df[['date', 'closeadj']]\n",
    "# original['date']=pd.to_datetime(original['date'])\n",
    "# original = original.loc[original['date'] >= '2020-5-1']\n",
    "\n",
    "# sns.lineplot(original['date'], original['closeadj'])\n",
    "# sns.lineplot(df_forecast['date'], df_forecast['closeadj'])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}